from model_embedding.model import ModelEmbedding
from src.llm import LLMModel
import os
import tempfile
import time
from src.loader import PDFLoader
from src.semantic_splitter import SemanticChunker
from vectordb.chroma import ChromaVectorStore
import streamlit as st
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain.prompts import PromptTemplate
from dotenv import load_dotenv
import logging

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv()
# Session state initialization
if 'rag_chain' not in st.session_state:
    st.session_state.rag_chain = None
if 'models_loaded' not in st.session_state:
    st.session_state.models_loaded = False
if 'embeddings' not in st.session_state:
    st.session_state.embeddings = None
if 'embedding_model' not in st.session_state:
    st.session_state.embedding_model = None
if 'llm' not in st.session_state:
    st.session_state.llm = None
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []
if 'pdf_processed' not in st.session_state:
    st.session_state.pdf_processed = False
if 'pdf_name' not in st.session_state:
    st.session_state.pdf_name = ""

@st.cache_resource
def load_embeddings():
    """Load HuggingFace embeddings for vector store"""
    try:
        model = ModelEmbedding("bkai-foundation-models/vietnamese-bi-encoder")
        return model.return_model()
    except Exception as e:
        logger.warning(f"Failed to load Vietnamese model, using backup: {e}")
        model = ModelEmbedding("all-MiniLM-L6-v2")
        return model.return_model()

@st.cache_resource
def load_embedding_model():
    """Load the actual embedding model object for SemanticChunker"""
    try:
        model = ModelEmbedding("bkai-foundation-models/vietnamese-bi-encoder")
        return model.model
    except Exception as e:
        logger.warning(f"Failed to load Vietnamese model, using backup: {e}")
        model = ModelEmbedding("all-MiniLM-L6-v2")
        return model.model

@st.cache_resource
def load_llm():
    """Load language model"""
    model_name = os.getenv("MODEL_NAME", "microsoft/DialoGPT-medium")
    try:
        llm_model = LLMModel(
            model_name=model_name,
            quantization=True
        )
        return llm_model.llm
    except Exception as e:
        logger.error(f"Failed to load LLM {model_name}: {e}")
        # Fallback to smaller model
        logger.info("Trying fallback model...")
        llm_model = LLMModel(
            model_name="microsoft/DialoGPT-small",
            quantization=True
        )
        return llm_model.llm


def process_pdf(uploaded_file):
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(uploaded_file.getvalue())
        tmp_file_path = tmp_file.name

    loader = PDFLoader(tmp_file_path)
    documents = loader.load()

    semantic_splitter = SemanticChunker(model_embedding = st.session_state.embedding_model)
    docs = semantic_splitter.split_documents(documents)

    vector_db = ChromaVectorStore(embedding_function = st.session_state.embeddings)
    vector_db.add_documents(docs)
    retriever = vector_db.as_retriever()

    # Create prompt template with fallback
    try:
        from langchain import hub
        prompt = hub.pull("rlm/rag-prompt")
    except Exception as e:
        logger.warning(f"Failed to pull prompt from hub, using fallback: {e}")
        template = """B·∫°n l√† tr·ª£ l√Ω AI th√¥ng minh. S·ª≠ d·ª•ng th√¥ng tin t·ª´ t√†i li·ªáu ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch ch√≠nh x√°c.

Th√¥ng tin t·ª´ t√†i li·ªáu:
{context}

C√¢u h·ªèi: {question}

H∆∞·ªõng d·∫´n:
- Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin t·ª´ t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p
- N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan, h√£y n√≥i "T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong t√†i li·ªáu"
- Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát
- Tr·∫£ l·ªùi ng·∫Øn g·ªçn v√† ch√≠nh x√°c

Tr·∫£ l·ªùi:"""
        prompt = PromptTemplate(
            input_variables=["context", "question"],
            template=template,
        )

    def format_docs(docs):
        return "\n\n".join([doc.page_content for doc in docs])
    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | st.session_state.llm
        | StrOutputParser()
    )

    os.unlink(tmp_file_path)
    return rag_chain, len(docs)

def add_message(role, content):
    """Th√™m tin nh·∫Øn v√†o l·ªãch s·ª≠ chat"""
    st.session_state.chat_history.append({
        "role": role,
        "content": content,
        "timestamp": time.time()
    })

def clear_chat():
    """X√≥a l·ªãch s·ª≠ chat"""
    st.session_state.chat_history = []

def display_chat():
    """Hi·ªÉn th·ªã l·ªãch s·ª≠ chat"""
    if st.session_state.chat_history:
        for message in st.session_state.chat_history:
            if message["role"] == "user":
                with st.chat_message("user"):
                    st.write(message["content"])
            else:
                with st.chat_message("assistant"):
                    st.write(message["content"])
    else:
        with st.chat_message("assistant"):
            st.write("Xin ch√†o! T√¥i l√† AI assistant. H√£y upload file PDF v√† b·∫Øt ƒë·∫ßu ƒë·∫∑t c√¢u h·ªèi v·ªÅ n·ªôi dung t√†i li·ªáu nh√©! üòä")

# UI
def main():
    st.set_page_config(
        page_title="PDF RAG Chatbot",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    # Header with emoji logo
    st.markdown("""
    <div style="text-align: center; padding: 1rem 0;">
        <h1>ü§ñ PDF RAG Assistant</h1>
        <p style="color: #666; margin-top: -10px;">Tr√≤ chuy·ªán th√¥ng minh v·ªõi t√†i li·ªáu PDF c·ªßa b·∫°n</p>
    </div>
    """, unsafe_allow_html=True)

    # Sidebar
    with st.sidebar:
        st.title("‚öôÔ∏è C√†i ƒë·∫∑t")

        # Load models
        if not st.session_state.models_loaded:
            st.warning("‚è≥ ƒêang t·∫£i models...")
            with st.spinner("ƒêang t·∫£i AI models..."):
                try:
                    st.session_state.embeddings = load_embeddings()
                    st.session_state.embedding_model = load_embedding_model()
                    st.session_state.llm = load_llm()
                    st.session_state.models_loaded = True
                    logger.info("All models loaded successfully!")
                except Exception as e:
                    st.error(f"L·ªói t·∫£i models: {str(e)}")
                    logger.error(f"Model loading failed: {e}")
                    st.stop()
            st.success("‚úÖ Models ƒë√£ s·∫µn s√†ng!")
            st.rerun()
        else:
            st.success("‚úÖ Models ƒë√£ s·∫µn s√†ng!")

        st.markdown("---")

        # Upload PDF
        st.subheader("üìÑ Upload t√†i li·ªáu")
        uploaded_file = st.file_uploader("Ch·ªçn file PDF", type="pdf")

        if uploaded_file:
            if st.button("üîÑ X·ª≠ l√Ω PDF", use_container_width=True):
                with st.spinner("ƒêang x·ª≠ l√Ω PDF..."):
                    st.session_state.rag_chain, num_chunks = process_pdf(uploaded_file)
                    st.session_state.pdf_processed = True
                    st.session_state.pdf_name = uploaded_file.name
                    # Reset chat history khi upload PDF m·ªõi
                    clear_chat()
                    add_message("assistant", f"‚úÖ ƒê√£ x·ª≠ l√Ω th√†nh c√¥ng file **{uploaded_file.name}**!\n\nüìä T√†i li·ªáu ƒë∆∞·ª£c chia th√†nh {num_chunks} ph·∫ßn. B·∫°n c√≥ th·ªÉ b·∫Øt ƒë·∫ßu ƒë·∫∑t c√¢u h·ªèi v·ªÅ n·ªôi dung t√†i li·ªáu.")
                st.rerun()

        # PDF status
        if st.session_state.pdf_processed:
            st.success(f"üìÑ ƒê√£ t·∫£i: {st.session_state.pdf_name}")
        else:
            st.info("üìÑ Ch∆∞a c√≥ t√†i li·ªáu")

        st.markdown("---")

        # Chat controls
        st.subheader("üí¨ ƒêi·ªÅu khi·ªÉn Chat")
        if st.button("üóëÔ∏è X√≥a l·ªãch s·ª≠ chat", use_container_width=True):
            clear_chat()
            st.rerun()

        st.markdown("---")

        # Instructions
        st.subheader("üìã H∆∞·ªõng d·∫´n")
        st.markdown("""
        **C√°ch s·ª≠ d·ª•ng:**
        1. **Upload PDF** - Ch·ªçn file v√† nh·∫•n "X·ª≠ l√Ω PDF"
        2. **ƒê·∫∑t c√¢u h·ªèi** - Nh·∫≠p c√¢u h·ªèi trong √¥ chat
        3. **Nh·∫≠n tr·∫£ l·ªùi** - AI s·∫Ω tr·∫£ l·ªùi d·ª±a tr√™n n·ªôi dung PDF
        """)

    # Main content
    st.markdown("*Tr√≤ chuy·ªán v·ªõi Chatbot ƒë·ªÉ trao ƒë·ªïi v·ªÅ n·ªôi dung t√†i li·ªáu PDF c·ªßa b·∫°n*")

    # Chat container
    chat_container = st.container()

    with chat_container:
        # Display chat history
        display_chat()

    # Chat input
    if st.session_state.models_loaded:
        if st.session_state.pdf_processed:
            # User input
            user_input = st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n...")

            if user_input:
                # Add user message
                add_message("user", user_input)

                # Display user message immediately
                with st.chat_message("user"):
                    st.write(user_input)

                # Generate response
                with st.chat_message("assistant"):
                    with st.spinner("ƒêang suy nghƒ©..."):
                        try:
                            output = st.session_state.rag_chain.invoke(user_input)
                            # Clean up the response
                            if 'Answer:' in output:
                                answer = output.split('Answer:')[1].strip()
                            else:
                                answer = output.strip()

                            # Display response
                            st.write(answer)

                            # Add assistant message to history
                            add_message("assistant", answer)

                        except Exception as e:
                            error_msg = f"Xin l·ªói, ƒë√£ c√≥ l·ªói x·∫£y ra: {str(e)}"
                            st.error(error_msg)
                            add_message("assistant", error_msg)
        else:
            st.info("üîÑ Vui l√≤ng upload v√† x·ª≠ l√Ω file PDF tr∆∞·ªõc khi b·∫Øt ƒë·∫ßu chat!")
            st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n...", disabled=True)
    else:
        st.info("‚è≥ ƒêang t·∫£i AI models, vui l√≤ng ƒë·ª£i...")
        st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n...", disabled=True)

if __name__ == "__main__":
    main()



